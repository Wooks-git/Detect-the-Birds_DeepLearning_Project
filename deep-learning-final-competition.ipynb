{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 서약\n",
    "* 당연한 것을 아래와 같이 한번 다지고 가도록 하겠습니다. 아래 보고서는 **본인의 힘만으로 작성**해야하며, 다른 수강생에게 직접적인 질문, 복사 하는 행위는 모두 금지합니다 \n",
    "  * 예를 들어서, 본 프로젝트의 코드 셀을 완성하는데 직접적인 질문 또는 복사하는 경우는 모두 금지합니다\n",
    "  * 수업에서 제공한 코드, 노트북은 모두 재활용가능하며, 카피로 규정하지 않습니다\n",
    "  * 수업 자료 이외에 참고자료가 있다면, 출처와 사용 부분에 모두 표시하는 경우는 모두 합당한 자료로 인정하겠습니다\n",
    "  \n",
    "* 위에 대해서 모두 이해하고 동의했다면, 아래 `서약글`에 다음을 작성해주세요:\n",
    "\n",
    "\"본인은 위 서약글을 이해하고 동의하며, 프로젝트를 수행하는데 있어서 반칙을 할 경우 (제공자 포함) 본 프로젝트에 대한 점수가 반영되지 않는다는 것에 동의 합니다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**서약서**\n",
    "\n",
    "이름: 박성욱\n",
    "\n",
    "학번: 20155319\n",
    "\n",
    "서약글:  본인은 위 서약글을 이해하고 동의하며, 프로젝트를 수행하는데 있어서 반칙을 할 경우(제공자 포함)본 프로젝트에 대한 점수가 반영되지 않는다는 것에 동의합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project \n",
    "\n",
    "## Project: 새 품종 분류하기\n",
    "\n",
    "## Due date: 2021.06.15\n",
    "\n",
    "---\n",
    "\n",
    "* 아래 여러 셀에서 코드를 완성하는 부분을 수행하고, 설명을 요구하는 부분은 보고서에 설명을 최대한 자세하게 적어주세요. 기준은 본인이 이해하고 있다는 것을 표현할 수 있는 부분을 모두 적으시면 됩니다.\n",
    "  * 답을 작성하는 원칙은 **보고서**를 작성한다고 생각하시면 됩니다\n",
    "  * 내가 알고 있는 부분을 충실하게 **글로 표현** 하는 것 또한 중요한 연습입니다 \n",
    "  * 코드 작성은 **주석**으로 설명하시기 바랍니다\n",
    "  \n",
    "  \n",
    "> **제출방법**: \n",
    "* 보고서에는 코드 캡쳐 첨부이외에도, 각 코드를 작성하는 방법론과 설명을 작성해야하는 **서술형 문제**도 포함되어 있습니다.\n",
    "* 서술형 문제는 채점하는 중요한 기준이 됩니다. 성실하게 작성해주세요.\n",
    "* 서술형 문제에 대한 답변은 첨부된 보고서에 작성하면 됩니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목표: 앱을 위한 인공지능 알고리즘 개발 ^ㅡ^\n",
    "* 본 보고서에서는 모바일/웹앱을 위한 인공지능 알고리즘을 개발하라는 업무를 부여받았다고 가정합니다\n",
    "* 프로젝트가 완성본은, 사용자가 제공하는 image를 받아서 새의 종(種)을 예측합니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import library\n",
    "\n",
    "- 필요하다 생각되는 라이브러리를 미리 import해 놓은 항목입니다.(참고)\n",
    "- 필요 없는 라이브러리를 제거하거나 필요한 라이브러리를 추가하셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T13:29:08.754525Z",
     "iopub.status.busy": "2021-05-26T13:29:08.754157Z",
     "iopub.status.idle": "2021-05-26T13:29:10.138939Z",
     "shell.execute_reply": "2021-05-26T13:29:10.138136Z",
     "shell.execute_reply.started": "2021-05-26T13:29:08.754493Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T13:29:12.321994Z",
     "iopub.status.busy": "2021-05-26T13:29:12.321671Z",
     "iopub.status.idle": "2021-05-26T13:29:13.213636Z",
     "shell.execute_reply": "2021-05-26T13:29:13.212815Z",
     "shell.execute_reply.started": "2021-05-26T13:29:12.321958Z"
    }
   },
   "outputs": [],
   "source": [
    "#코드작성\n",
    "#data의 경로를 저장한 변수\n",
    "path = 'C:/Jupyter Notebook/DL_homework/final_project/detect-the-bird/imgs/'\n",
    "\n",
    "#glob을 통하여 train image, test image를 불러올 경로를 각각 train_files, test_path의 리스트안에 저장\n",
    "# train_files = glob.glob(path + 'train/*/*.*')#####\n",
    "test_path = glob.glob(path + 'test/*')\n",
    "\n",
    "#train과 test data가 손실없이 정확히 불러와졌는지 check\n",
    "# print('train_data : ', len(train_files))#####\n",
    "print('test_data : ', len(test_path))\n",
    "\n",
    "#cuda\n",
    "cuda_ = torch.cuda.is_available()\n",
    "\n",
    "#cuda를 사용할 수 있으면 device는 cuda로 아니면 cpu사용\n",
    "if cuda_:\n",
    "    device = 'cuda'\n",
    "    print('device : ', device)\n",
    "    \n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('device : ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input data가 얼만큼 들어갈지 설정할 batch_size 설정\n",
    "batch_size = 20\n",
    "#이미지의 크기를 고정해주기 위한 image_size 설정\n",
    "image_size = 256\n",
    "\n",
    "#train data를 불러올 path 설정\n",
    "train_path = path + 'train'\n",
    "\n",
    "#train data의 mean, std를 구하기 위한 transform\n",
    "transform_train = transforms.Compose([transforms.Resize((image_size,image_size)),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imagefolder를 통해 각각의 폴더별로 클래스를 생성하고 그에 맞는 label을 할당\n",
    "train_folder = datasets.ImageFolder(train_path, transform = transform_train)\n",
    "#input으로 집어넣을 수 있도록 loader구성\n",
    "train_loader = torch.utils.data.DataLoader(train_folder, batch_size = batch_size, num_workers = 4, shuffle = True)\n",
    "\n",
    "#예측을 할 때 사용할 classes미리 선언\n",
    "classes = train_folder.classes\n",
    "\n",
    "#classes가 빠짐없이 잘 되어있는지 확인\n",
    "print('num_class : ', len(classes))\n",
    "print('classes : ', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 data에 정규화를 시켜줄 때 각각 RGB의 평균과 표준편차로 transform하기 때문에 각각 RGB의 평균을 내줌\n",
    "meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x,_ in train_loader]\n",
    "stdRGB = [np.std(x.numpy(), axis=(1,2)) for x,_ in train_loader]\n",
    "\n",
    "meanR = np.mean([m[0] for m in meanRGB])\n",
    "meanG = np.mean([m[1] for m in meanRGB])\n",
    "meanB = np.mean([m[2] for m in meanRGB])\n",
    "\n",
    "stdR = np.mean([s[0] for s in stdRGB])\n",
    "stdG = np.mean([s[1] for s in stdRGB])\n",
    "stdB = np.mean([s[2] for s in stdRGB])\n",
    "\n",
    "print(meanR, meanG, meanB)\n",
    "print(stdR, stdG, stdB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #validation size\n",
    "# valid_size = 0.3\n",
    "\n",
    "# #train data의 total 개수 저장\n",
    "# num_train = len(train_folder)\n",
    "\n",
    "# #train data의 개수만큼 idx_list라는 변수에 리스트로 설정 train은 총 2468개 이므로 0~2467로 이루어진 리스트 형성\n",
    "# idx_list = list(range(num_train))\n",
    "\n",
    "# #리스트를 섞어줌\n",
    "# np.random.shuffle(idx_list)\n",
    "\n",
    "# #위에 정해준 validation 크기만큼 train과 validation을 위한 idx split\n",
    "# split = int(np.floor(valid_size*num_train))\n",
    "# train_idx, valid_idx = idx_list[split:], idx_list[:split]\n",
    "\n",
    "# #위에서 나눈 train idx와 valid idx로 각각 train data와 validation data를 할당\n",
    "# train_sampler = SubsetRandomSampler(train_idx)\n",
    "# valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #data가 잘 나누어졌는지 확인\n",
    "# print('train data : ' ,len(train_sampler))\n",
    "# print('valid data : ', len(valid_sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-26T14:01:12.685144Z",
     "iopub.status.busy": "2021-05-26T14:01:12.684799Z",
     "iopub.status.idle": "2021-05-26T14:01:12.724877Z",
     "shell.execute_reply": "2021-05-26T14:01:12.723987Z",
     "shell.execute_reply.started": "2021-05-26T14:01:12.685107Z"
    }
   },
   "outputs": [],
   "source": [
    "#train data에 적용할 transform\n",
    "transform_train = transforms.Compose([transforms.Resize((image_size,image_size)), #image의 size를 위에서 정한 image size로 resize\n",
    "                                      transforms.CenterCrop(224), #resize한 이미지를 224x224로 잘라서 input에 넣는다. 영상의 가장자리보다 중심에 더 중요한\n",
    "                                                                  #정보가 있을 것으로 예상하여서\n",
    "                                      transforms.RandomHorizontalFlip(p=0.5), #0.5의 확률로 뒤집기\n",
    "                                      transforms.RandomRotation(10), #이미지를 10도 돌린다.\n",
    "                                      transforms.RandomGrayscale(p=0.1),#0.1의 확률로 흑백 이미지로 변환\n",
    "                                     transforms.ToTensor(), #데이터 타입을\n",
    "                                     transforms.Normalize([meanR, meanG, meanB],\n",
    "                                                            [stdR, stdG, stdB])])\n",
    "\n",
    "transform_valid = transforms.Compose([transforms.Resize((image_size, image_size)),\n",
    "                                    transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                     transforms.Normalize([meanR, meanG, meanB],\n",
    "                                                            [stdR, stdG, stdB])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Dataset 에 대한 Data Loaders 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #코드작성\n",
    "train_folder = datasets.ImageFolder(train_path, transform = transform_train)\n",
    "# valid_folder = datasets.ImageFolder(train_path, transform = transform_valid)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_folder, batch_size = batch_size, num_workers = 4, sampler = train_sampler)\n",
    "# valid_loader = torch.utils.data.DataLoader(valid_folder, batch_size = batch_size, num_workers = 4, sampler = valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Neural Network 생성\n",
    "- Pretrained model을 허용하지 않습니다. (직접 모델을 설계해 주세요)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3,64,7,1,padding = 3), #224\n",
    "                            nn.BatchNorm2d(64),\n",
    "                            nn.ReLU(True),\n",
    "                            nn.MaxPool2d(2,2)) #112\n",
    "\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(64,128,1,1,padding = 0), #112\n",
    "                                    nn.BatchNorm2d(128),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.Conv2d(128,256,3,1,padding = 1), #112\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2),#56\n",
    "                                    nn.Conv2d(256,128,1,1,padding = 0),\n",
    "                                    nn.BatchNorm2d(128),\n",
    "                                    nn.ReLU(True))\n",
    "\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(128,256,1,1,padding = 0),\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.Conv2d(256,512,3,1,padding = 1), \n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2),#28\n",
    "                                    nn.Conv2d(512,256,1,1,padding = 0),\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.ReLU(True))\n",
    "\n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(256,512,1,1,padding = 0),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.Conv2d(512,1024,3,1,padding = 1), \n",
    "                                    nn.BatchNorm2d(1024),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2),#14\n",
    "                                    nn.Conv2d(1024,512,1,1,padding = 0),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(True))\n",
    "\n",
    "        self.layer5 = nn.Sequential(nn.Conv2d(512,1024,1,1,padding = 0),\n",
    "                                    nn.BatchNorm2d(1024),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.Conv2d(1024,2048,3,1,padding = 1),\n",
    "                                    nn.BatchNorm2d(2048),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2),#7\n",
    "                                    nn.Conv2d(2048,1024,1,1,padding = 0),\n",
    "                                    nn.BatchNorm2d(1024),\n",
    "                                    nn.ReLU(True))\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Dropout(0.8),\n",
    "                                nn.Linear(1024*7*7, 1000),\n",
    "                                nn.BatchNorm1d(1000),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Dropout(0.8),\n",
    "                                nn.Linear(1000, 500),\n",
    "                                nn.BatchNorm1d(500),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Dropout(0.8),\n",
    "                                nn.Linear(500, 250),\n",
    "                                nn.BatchNorm1d(250),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(250, len(classes))\n",
    "                               )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    #모델의 forward연산\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        #flatten\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# model 생성\n",
    "model = model6()\n",
    "#GPU연산\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 4: Cost (Loss) Function 과 Optimizer 선택\n",
    " \n",
    " [loss function](http://pytorch.org/docs/stable/nn.html#loss-functions) 및 [optimizer](http://pytorch.org/docs/stable/optim.html)를 선택하여 코드를 완성하세요.\n",
    " \n",
    " 위 링크에서 다양한 Loss Function과 Optimize Function을 확인 할 수 있습니다\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드작성\n",
    "#crossentropy loss 사용\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#adam optimizer사용\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-2)\n",
    "\n",
    "#원하는 epoch에 사용할 scheduler 사용 주로 원하는 epoch에 lr을 감소시킬 수 있는 scheduler를 사용\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1)\n",
    "# scheduler_down = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 20, eta_min = 1e-3)\n",
    "# scheduler_up = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50], gamma=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 구성한 모델에 대한 Train and Validate 진행\n",
    "\n",
    "* 코드 전체를 주석으로 설명하세요\n",
    "* Epoch 별로 Loss나 Accuracy를 출력하여 학습 진행 과정을 확인 할 수 있도록 합니다\n",
    "* 출력 예시는 주어지나 정해진 형식은 없습니다\n",
    "* 최적의 모델 저장\n",
    "\n",
    "예제:\n",
    "```\n",
    "Started Training...\n",
    "Epoch: 1 \tTraining Loss: 3.317162 \tValidation Loss: 4.162958\n",
    "Epoch: 2 \tTraining Loss: 2.420140 \tValidation Loss: 4.182362\n",
    "...\n",
    "...\n",
    "Finished training\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model의 weight가 적용될 변수 선언\n",
    "pt_file = 'model weight'\n",
    "\n",
    "#몇 번의 epoch을 돌것인가\n",
    "n_epochs = 200\n",
    "\n",
    "#만약 20번 동안 weight save가 되지 않으면 해당 학습을 강제로 종료\n",
    "n_epochs_stop = 20\n",
    "early_stop = False\n",
    "\n",
    "#train data를 5분할 하여 각각 한 번씩 validation set으로 사용\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "#각각 train index와 validation index를 받아옴\n",
    "for fold, (train_ind, valid_ind) in enumerate(kf.split(train_folder)):\n",
    "    \n",
    "    #index를 통해 train data와 validation data를 나눠줌\n",
    "    train_sampler_kfold = SubsetRandomSampler(train_ind)\n",
    "    valid_sampler_kfold = SubsetRandomSampler(valid_ind)\n",
    "    \n",
    "    #전체 data 수, train data, validation data의 수를 확인\n",
    "    print(f'Total data : {len(train_folder)} Train data : {len(train_sampler_kfold)} Valid data : {len(valid_sampler_kfold)}')    \n",
    "    \n",
    "    #각각의 transform 적용을 위한 코드\n",
    "    train_folder_kfold = datasets.ImageFolder(train_path, transform = transform_train)\n",
    "    valid_folder_kfold = datasets.ImageFolder(train_path, transform = transform_valid)\n",
    "    \n",
    "    #train loader\n",
    "    train_loader_kfold = torch.utils.data.DataLoader(train_folder, batch_size=batch_size,\n",
    "    sampler=train_sampler_kfold, num_workers=4)\n",
    "    \n",
    "    #validation loader\n",
    "    valid_loader_kfold = torch.utils.data.DataLoader(valid_folder, batch_size=batch_size, \n",
    "    sampler=valid_sampler_kfold, num_workers=4)\n",
    "    \n",
    "#     valid_loader_kfold = transform_valid(valid_loader_kfold)\n",
    "    \n",
    "    #각 fold별로 새로 학습하기 위해 model을 선언\n",
    "    model = model6()\n",
    "    #gpu연산을 위함.\n",
    "    model.to(device)\n",
    "    \n",
    "    #loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #Adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 2e-2)\n",
    "    #scheduler\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80], gamma=0.1)\n",
    "    \n",
    "    #earlystop을 사용하고 싶으면 해당 부분에서 True로 아니면 False\n",
    "    earlystop = True\n",
    "    \n",
    "    #scheduler를 사용하고 싶으면 해당 부분에서 True로 아니면 False\n",
    "    scheduler_ = True\n",
    "    \n",
    "    #weight save가 얼마나 안됐는지 체크를 하여 early stop\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    #학습괴정에서의 train, validation loss를 저장\n",
    "    train_loss = torch.zeros(n_epochs)\n",
    "    valid_loss = torch.zeros(n_epochs)\n",
    "\n",
    "    #학습 과정에서의 train, validation accuracy를 저장\n",
    "    train_acc = torch.zeros(n_epochs)\n",
    "    valid_acc = torch.zeros(n_epochs)\n",
    "    \n",
    "    #validation loss를 통해 이전 epoch과 비교하여 줄어들었으면 weight를 저장하기 위한 변수\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    #선언한 epoch만큼 for문을 반복\n",
    "    for e in range(0, n_epochs):   \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "\n",
    "        #현재 epoch의 learning rate를 확인하기 위해서 scheduler 적용시 잘 적용이 되었나 확인하기 위함이다.\n",
    "        for param_group in optimizer.param_groups:\n",
    "                train_lr = param_group['lr']\n",
    "\n",
    "        #train mode로 전환\n",
    "        model.train()\n",
    "\n",
    "        #train loader를 통해 input data와 labels를 batch size만큼 떼어옴\n",
    "        for data, labels in tqdm(train_loader_kfold): \n",
    "            \n",
    "            #받아온 data와 labels를 gpu연산을 하기 위한 코드\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            #학습 과정에서 미분을 통해 얻은 기울기를 0으로 초기화\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #data를 model에 input으로 넣어 output 출력\n",
    "            logits = model(data)\n",
    "            \n",
    "            logits = logits.squeeze()\n",
    "            \n",
    "            #예측값과 label값을 비교하여 loss를 계산\n",
    "            loss = criterion(logits, labels)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            \n",
    "            #loss를 통해 기울기를 계산한다.\n",
    "            loss.backward() #Expected input batch_size (49) to match target batch_size (64)오류 발생\n",
    "            #계산한 기울기를 통해 learning rate만큼 weight를 업데이트\n",
    "            optimizer.step()\n",
    "\n",
    "            #train loss를 업데이트\n",
    "            train_loss[e] += loss.item()\n",
    "\n",
    "            #softmax를 통해 확률값을 구한다.\n",
    "            ps = F.softmax(logits, dim=1)\n",
    "            #구한 확률값중 가장 큰 값을 top_class에 적용\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            \n",
    "            #true label과 predict label을 비교하여 정확도를 계산\n",
    "            equals = top_class == labels.reshape(top_class.shape)\n",
    "            train_acc[e] += torch.mean(equals.type(torch.float)).detach().cpu()\n",
    "\n",
    "        #train loss와 train accuracy를 계산\n",
    "        train_loss[e] /= len(train_loader_kfold)\n",
    "        train_acc[e] /= len(train_loader_kfold)\n",
    "\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        \n",
    "        #autograd를 종료\n",
    "        with torch.no_grad():\n",
    "            #evaluation mode로 전환\n",
    "            model.eval()\n",
    "            #validation dataset을 받아옴\n",
    "            for data, labels in tqdm(valid_loader_kfold):\n",
    "                #data와 label을 gpu연산을 할 수 있도록\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                #valiation set을 input으로 넣어 out\n",
    "                logits = model(data)\n",
    "                #loss함수를 통해 loss를 계산\n",
    "                loss = criterion(logits, labels)\n",
    "                #validation loss를 추가해줌\n",
    "                valid_loss[e] += loss.item()\n",
    "                #모델의 output을 확률값으로 변경해준다.\n",
    "                ps = F.softmax(logits, dim=1)\n",
    "                #구한 확률값중 가장 큰 값을 top_class에 적용한다.\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                #true label과 predict label을 비교하여 정확도를 계산\n",
    "                equals = top_class == labels.reshape(top_class.shape)\n",
    "                valid_acc[e] += torch.mean(equals.type(torch.float)).detach().cpu()\n",
    "\n",
    "        #validation loss와 accuracy를 계산\n",
    "        valid_loss[e] /= len(valid_loader_kfold)\n",
    "        valid_acc[e] /= len(valid_loader_kfold)\n",
    "\n",
    "        #scheduler batchsize때문에 원하는 epoch에 lr이 줄어들지 않아 해당 위치에 배치\n",
    "        if scheduler_:\n",
    "            scheduler.step()\n",
    "\n",
    "        #현재 학습의 폴드와 기본 세팅값을 표기해줌\n",
    "        print(f'Fold : {fold} Learning rate : {train_lr:.6f}  no_improve_epoch : {epochs_no_improve}  Early Stop : {earlystop}   Scheduler : {scheduler_}')\n",
    "\n",
    "        # train 상태를 출력해줌\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            e+1, train_loss[e], valid_loss[e]))\n",
    "\n",
    "        #validation 상태를 출력해줌 \n",
    "        print('Epoch: {} \\tTraining accuracy: {:.6f}% \\tValidation accuracy: {:.6f}%'.format(\n",
    "            e+1, train_acc[e]*100, valid_acc[e]*100))\n",
    "\n",
    "        #이전의 validation loss와 비교하여 크거나 같으면 model weight를 저장\n",
    "        if valid_loss[e] <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss[e]))\n",
    "            torch.save(model.state_dict(), f'{pt_file}_{fold}.pt')\n",
    "            valid_loss_min = valid_loss[e]\n",
    "\n",
    "            #early stop 초기화\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            #weight save가 되지 않으면 early stop count를 하나씩 추가\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #early stop\n",
    "        if earlystop:\n",
    "            if e > 5 and epochs_no_improve == n_epochs_stop:\n",
    "                print('Early stopping!')\n",
    "                early_stop = True\n",
    "                break\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: CNN model training/validation 분석\n",
    "   * tranining loss와 validation loss 그래프를 통해서 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mtrain, validation loss plot\n",
    "plt.plot(train_loss, label = 'Training loss')\n",
    "plt.plot(valid_loss, label = 'validation loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'result_.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 최적의 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fold n번째 weight를 불러오기 위한 모델들 생성\n",
    "fold1 = model6()\n",
    "\n",
    "#fold1 weight load\n",
    "fold1.load_state_dict(torch.load('model weight_0.pt'))\n",
    "#gpu연산\n",
    "fold1.to(device)\n",
    "#evaluation mode로 전환\n",
    "fold1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold2 = model6()\n",
    "\n",
    "fold2.load_state_dict(torch.load('model weight_1.pt'))\n",
    "fold2.to(device)\n",
    "fold2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold3 = model6()\n",
    "\n",
    "fold3.load_state_dict(torch.load('model weight_2.pt'))\n",
    "fold3.to(device)\n",
    "fold3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold4 = model6()\n",
    "\n",
    "fold4.load_state_dict(torch.load('model weight_3.pt'))\n",
    "fold4.to(device)\n",
    "fold4.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold5 = model6()\n",
    "\n",
    "fold5.load_state_dict(torch.load('model weight_4.pt'))\n",
    "fold5.to(device)\n",
    "fold5.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Predict with Test Data \n",
    "\n",
    "\n",
    "### 예시 코드를 제공해 드립니다. 필요한 부분을 채워 사용하시거나 직접 코드를 작성 하셔도 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_dataset(Dataset):\n",
    "    def __init__(self,imgpath,transform=None):\n",
    "        #img path와 transform의 상태를 받아옴\n",
    "        self.imgpath = imgpath\n",
    "        self.transform = transform\n",
    "        #test img의 개수\n",
    "    def __len__(self):\n",
    "        return len(self.imgpath)\n",
    "        #데이터를 받아옴\n",
    "    def __getitem__(self,idx):\n",
    "        #transform적용\n",
    "        x = self.transform(Image.open(self.imgpath[idx]).convert('RGB'))\n",
    "        #gpu연산\n",
    "        x = x.to(device)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transform_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test dataset과 dataloader 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_dataset(test_path, transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = 1, num_workers = 0, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class e1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3,64,7,1,padding = 3), #224\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2)) #112\n",
    "        \n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(64,64,1,1,padding = 0), #112\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.Conv2d(64,64,3,1,padding = 1), #112\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2), #56\n",
    "                                    nn.Conv2d(64,256, 1,1,padding = 0), \n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2)) #28\n",
    "        \n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(256, 256, 1, 1, padding = 0), #28\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.Conv2d(256, 512, 3,1,padding =1), #28\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2), #14\n",
    "                                    nn.Conv2d(512, 1024, 3, 1, padding =1),\n",
    "                                    nn.BatchNorm2d(1024),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2)) #7\n",
    "        \n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(1024, 1024, 1, 1, padding = 0),\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.Conv2d(1024, 2048, 3, 1, padding = 1),\n",
    "                                    nn.BatchNorm2d(2048),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.Conv2d(2048, 1024, 1, 1, padding = 0),\n",
    "                                    nn.BatchNorm2d(1024),\n",
    "                                    nn.ReLU(True))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Dropout(0.8),\n",
    "                                nn.Linear(1024*7*7, 1000),\n",
    "                                nn.BatchNorm1d(1000),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Dropout(0.8),\n",
    "                                nn.Linear(1000, 500),\n",
    "                                nn.BatchNorm1d(500),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Dropout(0.8),\n",
    "                                nn.Linear(500, 250),\n",
    "                                nn.BatchNorm1d(250),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(250, len(classes))\n",
    "                                )\n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class e2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3,64,7,1,padding = 3), #224\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2)) #112\n",
    "        \n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(64,64,1,1,padding = 0), #112\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.Conv2d(64,64,3,1,padding = 1), #112\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2), #56\n",
    "                                    nn.Conv2d(64,256, 1,1,padding = 0), \n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2)) #28\n",
    "        \n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(256, 256, 1, 1, padding = 0), #28\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.Conv2d(256, 512, 3,1,padding =1), #28\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2), #14\n",
    "                                    nn.Conv2d(512, 1024, 3, 1, padding =1),\n",
    "                                    nn.BatchNorm2d(1024),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2)) #7\n",
    "        \n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(1024, 1024, 1, 1, padding = 0),\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.Conv2d(1024, 2048, 3, 1, padding = 1),\n",
    "                                    nn.BatchNorm2d(2048),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.Conv2d(2048, 1024, 1, 1, padding = 0),\n",
    "                                    nn.BatchNorm2d(1024),\n",
    "                                    nn.ReLU(True))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Dropout(0.9),\n",
    "                                nn.Linear(1024*7*7, 1000),\n",
    "                                nn.BatchNorm1d(1000),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Dropout(0.9),\n",
    "                                nn.Linear(1000, 500),\n",
    "                                nn.BatchNorm1d(500),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Dropout(0.9),\n",
    "                                nn.Linear(500, 250),\n",
    "                                nn.BatchNorm1d(250),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(250, len(classes))\n",
    "                                )\n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class e3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3,64,7,1,padding = 3), #224\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2)) #112\n",
    "        \n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(64,64,1,1,padding = 0), #112\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.Conv2d(64,64,3,1,padding = 1), #112\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2), #56\n",
    "                                    nn.Conv2d(64,256, 1,1,padding = 0), \n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2)) #28\n",
    "        \n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(256, 256, 1, 1, padding = 0), #28\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.Conv2d(256, 512, 3,1,padding =1), #28\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2), #14\n",
    "                                    nn.Conv2d(512, 1024, 3, 1, padding =1),\n",
    "                                    nn.BatchNorm2d(1024),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.MaxPool2d(2,2)) #7\n",
    "        \n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(1024, 1024, 1, 1, padding = 0),\n",
    "                                    nn.BatchNorm2d(1024),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.Conv2d(1024, 2048, 3, 1, padding = 1),\n",
    "                                    nn.BatchNorm2d(2048),\n",
    "                                    nn.ReLU(True),\n",
    "                                    nn.Conv2d(2048, 1024, 1, 1, padding = 0),\n",
    "                                    nn.BatchNorm2d(1024),\n",
    "                                    nn.ReLU(True))\n",
    "\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Dropout(0.8),\n",
    "                                nn.Linear(1024*7*7, 1000),\n",
    "                                nn.BatchNorm1d(1000),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Dropout(0.8),\n",
    "                                nn.Linear(1000, 500),\n",
    "                                nn.BatchNorm1d(500),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Dropout(0.8),\n",
    "                                nn.Linear(500, 250),\n",
    "                                nn.BatchNorm1d(250),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(250, len(classes))\n",
    "                                )\n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble\n",
    "em1 = e1()\n",
    "\n",
    "em1.load_state_dict(torch.load('model4_77.pt'))\n",
    "em1.to(device)\n",
    "em1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em2 = e2()\n",
    "em2.load_state_dict(torch.load('model_5_80.pt'))\n",
    "em2.to(device)\n",
    "\n",
    "em2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em3 = e3()\n",
    "em3.load_state_dict(torch.load('model6_83_1.pt'))\n",
    "em3.to(device)\n",
    "\n",
    "em3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em4 = e2()\n",
    "em4.load_state_dict(torch.load('model5_71.pt'))\n",
    "em4.to(device)\n",
    "\n",
    "em4.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em5 = e3()\n",
    "em5.load_state_dict(torch.load('model6_81_45.pt'))\n",
    "em5.to(device)\n",
    "\n",
    "em5.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=[]\n",
    "#test loader를 통해 test image를 받아온다.\n",
    "for data in test_loader:\n",
    "    #test image를 input에 적용\n",
    "    logits1 = fold1(data)\n",
    "    logits2 = fold2(data)\n",
    "    logits3 = fold3(data)\n",
    "    logits4 = fold4(data)\n",
    "    logits5 = fold5(data)\n",
    "    logits6 = em1(data)\n",
    "    logits7 = em2(data)\n",
    "    logits8 = em3(data)\n",
    "    logits9 = em4(data)\n",
    "    logits10 = em5(data)\n",
    "\n",
    "    #model을 통과한 클래스들의 확률값을 구해준다.\n",
    "    ps1 = F.softmax(logits1, dim=1)\n",
    "    ps2 = F.softmax(logits2, dim=1)\n",
    "    ps3 = F.softmax(logits3, dim=1)\n",
    "    ps4 = F.softmax(logits4, dim=1)\n",
    "    ps5 = F.softmax(logits5, dim=1)\n",
    "    ps6 = F.softmax(logits6, dim=1)\n",
    "    ps7 = F.softmax(logits7, dim=1)\n",
    "    ps8 = F.softmax(logits8, dim=1)\n",
    "    ps9 = F.softmax(logits9, dim=1)\n",
    "    ps10 = F.softmax(logits10, dim=1)\n",
    "\n",
    "    #모든 확률값의 평균을 적용하여 확률을 앙상블\n",
    "    ps = (ps1+ps2+ps3+ps4+ps5+ps6+ps7+ps8+ps9+ps10)/10\n",
    "    \n",
    "     #확률값중 가장 높은 확률의 class를 return\n",
    "    _, top_class = ps.topk(1, dim =1)\n",
    "    \n",
    "    #class를 정수형으로 바꿔 0~14의 값으로 변환해준다.\n",
    "    pred += [int(top_class)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측 결과 인덱스를 저장한 pred 값을 사용해 클래스를 매칭하기 위한 class_name을 담아 놓은 리스트입니다.\n",
    "\n",
    "주의할 점은 Train과정에서 사용된 class lable값과 같은 순서로 저장이 되어 있어야 한다는 점입니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=[]\n",
    "category =[]\n",
    "\n",
    "for i in range(len(test_path)):\n",
    "    id.append(test_path[i].split('/')[-1].split('\\\\')[-1])\n",
    "    category.append(classes[pred[i]])\n",
    "pd.DataFrame({'Id':id,'Category':category}).to_csv('submission1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
